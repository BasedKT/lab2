{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "from math import exp\n",
    "\n",
    "Methods = Enum('Methods', ['Classic', 'Momentum', 'AdaGrad', 'RMSprop', 'Adam', 'Nesterov'])\n",
    "Regularization = Enum('Regularization', ['WithoutRegularization', 'L1', 'L2', 'Elastic'])\n",
    "LearningRate = Enum('LearningRate', ['Const', 'Dichotomy'])\n",
    "LearningRateScheduling = Enum('LearningRateScheduling', ['Classic', 'Stepwise', 'Exponential'])\n",
    "\n",
    "\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, T, W, X, Y, regularization=Regularization.WithoutRegularization, l1=0.1, l2=0.1):\n",
    "        self.T = np.array([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))]).reshape(len(X), len(T))\n",
    "        self.W = W\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.regularization = regularization\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def sse(self, W_Arg):\n",
    "        val = sum([(np.dot(self.T[i], W_Arg) - self.Y[i]) ** 2 for i in range(len(self.X))])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                val += self.l1 * sum([abs(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                val += self.l2 * sum([w ** 2 for w in self.W]) / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                val += (self.l1 * sum([abs(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * sum([w ** 2 for w in self.W])) / len(self.W)\n",
    "        return val\n",
    "\n",
    "    def grad_by_components(self, index_components, W_Arg):\n",
    "        grad_with_batch = np.zeros(len(W_Arg))\n",
    "        for i in index_components:\n",
    "            grad_with_batch += 2 * (np.dot(self.T[i], W_Arg) - self.Y[i]) * self.T[i]\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                grad_with_batch += self.l1 * np.array([sign(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                grad_with_batch += self.l2 * 2 * self.W / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                grad_with_batch += (self.l1 * np.array([sign(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * 2 * self.W) / len(self.W)\n",
    "\n",
    "        return grad_with_batch\n",
    "\n",
    "\n",
    "def sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam, is_corr_beta_1=True,\n",
    "        is_corr_beta_2=True, is_nesterov=False, decay=0.99):\n",
    "    i = 0\n",
    "    prev_W = lin_reg.sse(lin_reg.W)\n",
    "    V = np.zeros(len(lin_reg.W))\n",
    "    S = np.zeros(len(lin_reg.W))\n",
    "    lrs_func = lrs_handler(lrs, decay)\n",
    "\n",
    "    while True:\n",
    "        components = [(i * batch + j) % len(lin_reg.X) for j in range(batch)]\n",
    "        cur_w = lin_reg.W\n",
    "        grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        alpha = lrs_func(lr(lambda a: lin_reg.sse(lin_reg.W - a * grad_with_batch)), (i * batch) % len(lin_reg.X))\n",
    "        if is_nesterov:\n",
    "            cur_w -= alpha * beta_1 * V\n",
    "            grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        V = (beta_1 * V) + (1 - beta_1) * grad_with_batch\n",
    "        S = (beta_2 * S) + (1 - beta_2) * (grad_with_batch ** 2)\n",
    "        if is_corr_beta_1:\n",
    "            V /= 1 - (beta_1 ** (i + 1))\n",
    "        if is_corr_beta_2:\n",
    "            S /= 1 - (beta_2 ** (i + 1))\n",
    "\n",
    "        lin_reg.W -= alpha * (V / ((S + eps_adam) ** 0.5))\n",
    "        if abs(lin_reg.sse(lin_reg.W) - prev_W) < eps or i >= max_num_of_step:\n",
    "            break\n",
    "        prev_W = lin_reg.sse(lin_reg.W)\n",
    "        i += 1\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def sgd_handler(lin_reg, lr, method, lrs=LearningRateScheduling.Classic, batch=1, beta_1=0.9, beta_2=0.999,\n",
    "                eps_adam=10 ** -8,\n",
    "                eps=0.001, max_num_of_step=10000):\n",
    "    args = {}\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, 0, 1, 1, False, False)\n",
    "        case Methods.Momentum:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, 1, 1, False, False)\n",
    "        case Methods.AdaGrad:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, 0, 0, eps_adam, False, False)\n",
    "        case Methods.RMSprop:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, 0, beta_2, eps_adam, False)\n",
    "        case Methods.Adam:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam)\n",
    "        case Methods.Nesterov:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, 1, 1, False, False, True)\n",
    "\n",
    "\n",
    "def lr_dichotomy(eps, delt):\n",
    "    return lambda lin_reg: dichotomy(lin_reg, 0, right_border_calc(lin_reg), eps, delt)\n",
    "\n",
    "\n",
    "def right_border_calc(func):\n",
    "    right_start = 0.0000001\n",
    "    zero = func(0.)\n",
    "    while zero >= func(right_start):\n",
    "        right_start *= 1.3\n",
    "\n",
    "    return right_start\n",
    "\n",
    "\n",
    "def dichotomy(func, a_1, a_2, eps, delt):\n",
    "    while abs(a_1 - a_2) >= eps:\n",
    "        new_a_1 = (a_1 + a_2) / 2 - delt\n",
    "        new_a_2 = (a_1 + a_2) / 2 + delt\n",
    "        fv1 = func(new_a_1)\n",
    "        fv2 = func(new_a_2)\n",
    "        if fv2 > fv1:\n",
    "            a_2 = new_a_2\n",
    "        elif fv2 < fv1:\n",
    "            a_1 = new_a_1\n",
    "        else:\n",
    "            a_1 = new_a_1\n",
    "            a_2 = new_a_2\n",
    "    return (a_1 + a_2) / 2\n",
    "\n",
    "\n",
    "def lrs_exp(decay=0.99):\n",
    "    return lambda lr, t: lr * (decay ** t)\n",
    "\n",
    "\n",
    "def lrs_step(decay=0.99):\n",
    "    return lambda lr, t: lr / (1 + decay * t)\n",
    "\n",
    "\n",
    "def lrs_handler(lrs, decay=0.99):\n",
    "    match lrs:\n",
    "        case LearningRateScheduling.Classic:\n",
    "            return lambda lr, t: lr\n",
    "        case LearningRateScheduling.Stepwise:\n",
    "            return lrs_step(decay)\n",
    "        case LearningRateScheduling.Exponential:\n",
    "            return lrs_exp(decay)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T18:29:49.008249Z",
     "end_time": "2023-04-13T18:29:49.082252Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualise_points(linear_reg):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = linear_reg.W[0] * x + linear_reg.W[1]\n",
    "    plt.plot(x, y, '-r')\n",
    "    plt.plot(linear_reg.X, linear_reg.Y, 'og', linestyle='None')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.show()\n",
    "    print('W: ' + str(linear_reg.W))\n",
    "    print('Loss: ' + str(linear_reg.sse(linear_reg.W)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T18:29:49.030244Z",
     "end_time": "2023-04-13T18:29:49.083252Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_t = np.array([lambda x: x, lambda x: 1.])\n",
    "current_w = np.array([0., 0.])\n",
    "current_x = np.array([1., 2., 9., -2., -10.])\n",
    "current_y = np.array([1., 2., 9., -2., 5.])\n",
    "\n",
    "for method in Methods:\n",
    "    for regularization in Regularization:\n",
    "        for lr in LearningRate:\n",
    "            for lrs in LearningRateScheduling:\n",
    "                linear_reg_const = LinearRegression(\n",
    "                    current_t, current_w, current_x, current_y, regularization\n",
    "                )\n",
    "                print(str(method), str(regularization), str(lr), str(lrs))\n",
    "                count_rates = 0\n",
    "                if lr == LearningRate.Const:\n",
    "                    count_rates = sgd_handler(linear_reg_const, lambda x: 0.01, method, lrs=lrs)\n",
    "                elif lr == LearningRate.Dichotomy:\n",
    "                    count_rates = sgd_handler(linear_reg_const, lr_dichotomy(0.001, 0.0001), method, lrs=lrs)\n",
    "                visualise_points(linear_reg_const)\n",
    "                print('Count rates: ' + str(count_rates))\n",
    "                current_w = np.array([0., 0.])\n",
    "                print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T18:29:49.047237Z",
     "end_time": "2023-04-13T18:31:06.706652Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T18:31:06.650549Z",
     "end_time": "2023-04-13T18:31:06.721601Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
