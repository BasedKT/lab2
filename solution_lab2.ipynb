{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "Methods = Enum('Methods', ['Classic', 'Momentum', 'AdaGrad', 'RMSprop', 'Adam', 'Nesterov'])\n",
    "Regularization = Enum('Regularization', ['WithoutRegularization', 'L1', 'L2', 'Elastic'])\n",
    "LearningRate = Enum('LearningRate', ['Const'])\n",
    "LearningRateScheduling = Enum('LearningRateScheduling', ['Classic', 'Stepwise', 'Exponential'])\n",
    "\n",
    "\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, T, W, X, Y, regularization=Regularization.WithoutRegularization, l1=0.1, l2=0.1):\n",
    "        self.T = np.array([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))]).reshape(len(X), len(T))\n",
    "        self.W = W\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.regularization = regularization\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.W_points = [np.copy(self.W)]\n",
    "        self.loss_values = [self.loss(self.W)]\n",
    "\n",
    "    def loss(self, W_Arg, is_avarage=False):\n",
    "        val = sum([(np.dot(self.T[i], W_Arg) - self.Y[i]) ** 2 for i in range(len(self.X))])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                val += self.l1 * sum([abs(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                val += self.l2 * sum([w ** 2 for w in self.W]) / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                val += (self.l1 * sum([abs(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * sum([w ** 2 for w in self.W])) / len(self.W)\n",
    "\n",
    "        return val / len(self.X) if is_avarage else val\n",
    "\n",
    "    def grad_by_components(self, index_components, W_Arg):\n",
    "        grad_with_batch = np.zeros(len(W_Arg))\n",
    "        for i in index_components:\n",
    "            grad_with_batch += (2 * (np.dot(self.T[i], W_Arg) - self.Y[i]) * self.T[i])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                grad_with_batch += self.l1 * np.array([sign(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                grad_with_batch += self.l2 * 2 * self.W / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                grad_with_batch += (self.l1 * np.array([sign(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * 2 * self.W) / len(self.W)\n",
    "\n",
    "        return grad_with_batch\n",
    "\n",
    "    def analytical_solution(self):\n",
    "        return (np.linalg.inv(np.transpose(self.T) @ self.T) @ np.transpose(self.T)) @ self.Y\n",
    "\n",
    "\n",
    "def sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam, is_corr_beta_1=True,\n",
    "        is_corr_beta_2=True, is_nesterov=False, is_adagrad=False, without_squares=False,\n",
    "        store_points=False):\n",
    "    i = 0\n",
    "    prev_W = lin_reg.loss(lin_reg.W)\n",
    "    V = np.zeros(len(lin_reg.W))\n",
    "    S = np.zeros(len(lin_reg.W))\n",
    "    lrs_func = lrs_handler(lrs)\n",
    "\n",
    "    while True:\n",
    "        i += 1\n",
    "\n",
    "        components = [(i * batch + j) % len(lin_reg.X) for j in range(batch)]\n",
    "        cur_w = lin_reg.W\n",
    "        grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        alpha = lrs_func(lr(lambda a: lin_reg.loss(lin_reg.W - a * grad_with_batch)), (i * batch) // len(lin_reg.X))\n",
    "        if is_nesterov:\n",
    "            cur_w -= alpha * beta_1 * V\n",
    "            grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        V = (beta_1 * V) + (1 - beta_1) * grad_with_batch\n",
    "        S = (beta_2 * S) + (1 - beta_2) * (grad_with_batch ** 2) if ~is_adagrad else (S + (grad_with_batch ** 2))\n",
    "        V_norm = V / (1 - (beta_1 ** (i + 1))) if is_corr_beta_1 else V\n",
    "        S_norm = S / (1 - (beta_2 ** (i + 1))) if is_corr_beta_2 else S\n",
    "\n",
    "        if without_squares:\n",
    "            lin_reg.W = lin_reg.W - alpha * V_norm\n",
    "        else:\n",
    "            lin_reg.W = lin_reg.W - alpha * (V_norm / (((S_norm) + eps_adam) ** 0.5))\n",
    "\n",
    "        if store_points:\n",
    "            lin_reg.W_points.append(np.copy(lin_reg.W))\n",
    "        lin_reg.loss_values.append(lin_reg.loss(lin_reg.W))\n",
    "        if abs(lin_reg.loss(lin_reg.W) - prev_W) < eps or i >= max_num_of_step:\n",
    "            break\n",
    "        prev_W = lin_reg.loss(lin_reg.W)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def sgd_handler(lin_reg, lr, method, lrs=LearningRateScheduling.Classic, batch=1, beta_1=0.9, beta_2=0.999,\n",
    "                eps_adam=10 ** -8,\n",
    "                eps=0.0001, max_num_of_step=10000, store_points=False):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, without_squares=True, store_points=store_points)\n",
    "        case Methods.Momentum:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, without_squares=True, store_points=store_points)\n",
    "        case Methods.AdaGrad:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=0.5, eps_adam=eps_adam,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, is_adagrad=True, store_points=store_points)\n",
    "        case Methods.RMSprop:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=beta_2, eps_adam=eps_adam,\n",
    "                       is_corr_beta_1=False, store_points=store_points)\n",
    "        case Methods.Adam:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam,\n",
    "                       store_points=store_points)\n",
    "        case Methods.Nesterov:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, is_nesterov=True, without_squares=True,\n",
    "                       store_points=store_points)\n",
    "\n",
    "\n",
    "def lrs_exp(decay, epoch_update):\n",
    "    return lambda lr, t: lr / (decay ** (t // epoch_update))\n",
    "\n",
    "\n",
    "def lrs_step(decay, epoch_update):\n",
    "    return lambda lr, t: lr - decay * (t // epoch_update)\n",
    "\n",
    "\n",
    "def lrs_handler(lrs, epoch_update=10):\n",
    "    match lrs:\n",
    "        case LearningRateScheduling.Classic:\n",
    "            return lambda lr, t: lr\n",
    "        case LearningRateScheduling.Stepwise:\n",
    "            return lrs_step(0.0001, epoch_update)\n",
    "        case LearningRateScheduling.Exponential:\n",
    "            return lrs_exp(2, epoch_update)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T15:45:44.902676Z",
     "end_time": "2023-04-16T15:45:44.924026Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualise_points(lin_reg):\n",
    "    x = np.linspace(min(lin_reg.X), max(lin_reg.X), 1000)\n",
    "    y = sum(\n",
    "        [lin_reg.W[i] * (x ** i) for i in range(len(lin_reg.W))]\n",
    "    )\n",
    "    analys_w = lin_reg.analytical_solution()\n",
    "    analys_y = sum(\n",
    "        [analys_w[i] * (x ** i) for i in range(len(analys_w))]\n",
    "    )\n",
    "    plt.plot(x, y, '-r')\n",
    "    plt.plot(x, analys_y, '-b')\n",
    "    plt.plot(lin_reg.X, lin_reg.Y, 'og', linestyle='None')\n",
    "    plt.legend(['Predict solution', 'Analytics solution', 'Train data'])\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualise_linear_sgd(lin_reg):\n",
    "    values = np.reshape(lin_reg.W_points, (2, len(lin_reg.W_points)))\n",
    "    X = np.linspace(min(values[0]) - 2, max(values[0]) + 2, 100)\n",
    "    Y = np.linspace(min(values[1]) - 2, max(values[1]) + 2, 100)\n",
    "    Z = [[lin_reg.loss(np.array((X[i], Y[j]))) for i in range(len(X))] for j in range(len(Y))]\n",
    "    plt.contour(X, Y, Z, 40)\n",
    "\n",
    "    plt.plot(values[0], values[1], marker='.')\n",
    "    plt.plot(values[0][0], values[1][0], 'og')\n",
    "    plt.plot(values[0][-1], values[1][-1], 'or')\n",
    "    plt.legend(['SGD', 'Start point', 'End point'])\n",
    "    plt.xlabel('w_1')\n",
    "    plt.ylabel('w_2')\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T15:45:44.932033Z",
     "end_time": "2023-04-16T15:45:44.940027Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "from excel import ExcellSaver\n",
    "\n",
    "\n",
    "def poly_array(coeffs):\n",
    "    return [lambda x, i=i: coeffs[i] * (x ** i) for i in range(len(coeffs))]\n",
    "\n",
    "\n",
    "def poly(poly_arr):\n",
    "    return lambda x: sum([poly_arr[i](x) for i in range(len(poly_arr))])\n",
    "\n",
    "\n",
    "def generate_data(num_of_points, dimension, coeffs_left, coeffs_right, x_left, x_right, deviation):\n",
    "    coeffs = np.array([float(random.randint(coeffs_left, coeffs_right)) for i in range(dimension + 1)])\n",
    "\n",
    "    X = [random.uniform(x_left, x_right) for _ in range(num_of_points)]\n",
    "    Y = [poly(poly_array(coeffs))(X[i]) + random.uniform(-deviation, +deviation) for i in range(num_of_points)]\n",
    "\n",
    "    return [np.array(X), np.array(Y), coeffs]\n",
    "\n",
    "\n",
    "def gen_linear_reg(dimension, num_of_points, coeffs_left, coeffs_right, x_left, x_right, deviation):\n",
    "    T = np.array(poly_array(np.ones(dimension + 1)))\n",
    "    X, Y, coeffs = generate_data(num_of_points, dimension, coeffs_left, coeffs_right, x_left, x_right, deviation)\n",
    "    W = np.ones(len(coeffs))\n",
    "\n",
    "    return LinearRegression(T, W, X, Y)\n",
    "\n",
    "\n",
    "def test_universal(lin_reg, lr, method, lrs, batch=1, store_points=False):\n",
    "    res_univ = {\n",
    "        'mem': 0,\n",
    "        'steps': 0,\n",
    "        'time': 0,\n",
    "        'error': 0\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "    steps = sgd_handler(lin_reg, lr, method, lrs=lrs, batch=batch, store_points=store_points)\n",
    "    res_univ['mem'] = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    end = time.time()\n",
    "\n",
    "    res_univ['steps'] = steps\n",
    "    res_univ['time'] = end - start\n",
    "    res_univ['error'] = lin_reg.loss(lin_reg.W, is_avarage=True)\n",
    "\n",
    "    return res_univ\n",
    "\n",
    "\n",
    "def refresh_lin_reg(lin_reg):\n",
    "    lin_reg.W = np.ones(len(lin_reg.W))\n",
    "    lin_reg.loss_values = [lin_reg.loss(lin_reg.W)]\n",
    "    lin_reg.W_points = [np.copy(lin_reg.W)]\n",
    "\n",
    "\n",
    "saver = ExcellSaver()\n",
    "\n",
    "\n",
    "def show_info(method, step, lrs, batch, results):\n",
    "    info = 'Method: {} | LR : {}, LRS: {}, Batch: {} | Steps: {}, Time (sec): {}, Error: {} | Mem: {}' \\\n",
    "        .format(method.name, step, lrs.name, batch, results['steps'], results['time'], results['error'], results['mem'])\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def batch_test():\n",
    "    print('-' * 50)\n",
    "    print('Batch Test')\n",
    "\n",
    "    saver.add_new_sheet(['Method', 'LR', 'LRS', 'Batch Size', 'Steps', 'Time (sec)', 'Error'], 'Batch Test')\n",
    "    num_of_points = 10\n",
    "    lin_reg = gen_linear_reg(dimension=1, num_of_points=num_of_points,\n",
    "                             coeffs_left=-2, coeffs_right=2, x_left=0., x_right=1., deviation=1.)\n",
    "\n",
    "    for method in Methods:\n",
    "        step = 0.001\n",
    "        lr = lambda *args: step\n",
    "        lrs = LearningRateScheduling.Classic\n",
    "        for batch in range(1, num_of_points + 1):\n",
    "            results = test_universal(lin_reg, lr, method, lrs, batch)\n",
    "            saver.add_row([method.name, step, lrs.name, batch, results['steps'], results['time'], results['error']])\n",
    "            show_info(method, step, lrs, batch, results)\n",
    "            visualise_points(lin_reg)\n",
    "            refresh_lin_reg(lin_reg)\n",
    "\n",
    "\n",
    "def lrs_test():\n",
    "    print('-' * 50)\n",
    "    print('LRS Test')\n",
    "\n",
    "    saver.add_new_sheet(['Method', 'LR', 'LRS', 'Batch Size', 'Steps', 'Time (sec)', 'Error'], 'LRS Test')\n",
    "    num_of_points = 100\n",
    "    lin_reg = gen_linear_reg(dimension=1, num_of_points=num_of_points,\n",
    "                             coeffs_left=-2, coeffs_right=2, x_left=0., x_right=1., deviation=1.)\n",
    "\n",
    "    for method in Methods:\n",
    "        step = 0.001\n",
    "        lr = lambda *args: step\n",
    "        for lrs in LearningRateScheduling:\n",
    "            for batch in [1, num_of_points // 3, num_of_points]:\n",
    "                results = test_universal(lin_reg, lr, method, lrs, batch)\n",
    "                saver.add_row([method.name, step, lrs.name, batch, results['steps'], results['time'], results['error']])\n",
    "                show_info(method, step, lrs, batch, results)\n",
    "                visualise_points(lin_reg)\n",
    "                refresh_lin_reg(lin_reg)\n",
    "\n",
    "\n",
    "def methods_test():\n",
    "    print('-' * 50)\n",
    "    print('Method Test')\n",
    "\n",
    "    saver.add_new_sheet(['Method', 'LR', 'LRS', 'Batch Size', 'Steps', 'Time (sec)', 'Error', 'Mem'], 'Method Test')\n",
    "    num_of_points = 100\n",
    "    lin_reg = gen_linear_reg(dimension=8, num_of_points=num_of_points,\n",
    "                             coeffs_left=-2, coeffs_right=2, x_left=0., x_right=1., deviation=1.)\n",
    "    step = 0.001\n",
    "    lr = lambda *args: step\n",
    "    for method in Methods:\n",
    "        for lrs in LearningRateScheduling:\n",
    "            for batch in [1, num_of_points // 3, num_of_points]:\n",
    "                results = test_universal(lin_reg, lr, method, lrs, batch)\n",
    "                saver.add_row([method.name, step, lrs.name, batch, results['steps'], results['time'], results['error'], results['mem']])\n",
    "                show_info(method, step, lrs, batch, results)\n",
    "                visualise_points(lin_reg)\n",
    "                refresh_lin_reg(lin_reg)\n",
    "\n",
    "\n",
    "def convergence_test():\n",
    "    print('-' * 50)\n",
    "    print('Convergence Test')\n",
    "\n",
    "    num_of_points = 50\n",
    "    lin_reg = gen_linear_reg(dimension=1, num_of_points=num_of_points,\n",
    "                             coeffs_left=-2, coeffs_right=0, x_left=-1., x_right=1., deviation=1.)\n",
    "    step = 0.001\n",
    "    lr = lambda *args: step\n",
    "    for method in Methods:\n",
    "        for lrs in LearningRateScheduling:\n",
    "            for batch in [1, num_of_points // 3, num_of_points]:\n",
    "                results = test_universal(lin_reg, lr, method, lrs, batch, store_points=True)\n",
    "                show_info(method, step, lrs, batch, results)\n",
    "                visualise_points(lin_reg)\n",
    "                lin_reg.W_points = lin_reg.W_points[::num_of_points // batch]\n",
    "                visualise_linear_sgd(lin_reg)\n",
    "                refresh_lin_reg(lin_reg)\n",
    "\n",
    "\n",
    "# batch_test()\n",
    "# lrs_test()\n",
    "# methods_test()\n",
    "convergence_test()\n",
    "# saver.create_excel()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T15:45:44.955035Z",
     "end_time": "2023-04-16T15:47:56.197266Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
