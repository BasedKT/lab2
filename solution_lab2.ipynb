{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "Methods = Enum('Methods', ['Classic', 'Momentum', 'AdaGrad', 'RMSprop', 'Adam', 'Nesterov'])\n",
    "Regularization = Enum('Regularization', ['WithoutRegularization', 'L1', 'L2', 'Elastic'])\n",
    "LearningRate = Enum('LearningRate', ['Const'])\n",
    "LearningRateScheduling = Enum('LearningRateScheduling', ['Classic', 'Stepwise', 'Exponential'])\n",
    "\n",
    "\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, T, W, X, Y, regularization=Regularization.WithoutRegularization, l1=0.1, l2=0.1):\n",
    "        self.T = np.array([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))]).reshape(len(X), len(T))\n",
    "        self.W = W\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.regularization = regularization\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.W_points = [np.copy(self.W)]\n",
    "        self.loss_values = [self.loss(self.W)]\n",
    "\n",
    "    def loss(self, W_Arg, is_avarage=False):\n",
    "        val = sum([(np.dot(self.T[i], W_Arg) - self.Y[i]) ** 2 for i in range(len(self.X))])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                val += self.l1 * sum([abs(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                val += self.l2 * sum([w ** 2 for w in self.W]) / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                val += (self.l1 * sum([abs(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * sum([w ** 2 for w in self.W])) / len(self.W)\n",
    "\n",
    "        return val / len(self.X) if is_avarage else val\n",
    "\n",
    "    def grad_by_components(self, index_components, W_Arg):\n",
    "        grad_with_batch = np.zeros(len(W_Arg))\n",
    "        for i in index_components:\n",
    "            grad_with_batch += (2 * (np.dot(self.T[i], W_Arg) - self.Y[i]) * self.T[i])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                grad_with_batch += self.l1 * np.array([sign(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                grad_with_batch += self.l2 * 2 * self.W / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                grad_with_batch += (self.l1 * np.array([sign(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * 2 * self.W) / len(self.W)\n",
    "\n",
    "        return grad_with_batch\n",
    "\n",
    "    def analytical_solution(self):\n",
    "        return (np.linalg.inv(np.transpose(self.T) @ self.T) @ np.transpose(self.T)) @ self.Y\n",
    "\n",
    "\n",
    "def sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam, is_corr_beta_1=True,\n",
    "        is_corr_beta_2=True, is_nesterov=False, is_adagrad=False, without_squares=False,\n",
    "        store_points=False):\n",
    "    i = 0\n",
    "    prev_W = lin_reg.loss(lin_reg.W)\n",
    "    V = np.zeros(len(lin_reg.W))\n",
    "    S = np.zeros(len(lin_reg.W))\n",
    "    lrs_func = lrs_handler(lrs)\n",
    "\n",
    "    while True:\n",
    "        i += 1\n",
    "\n",
    "        components = [(i * batch + j) % len(lin_reg.X) for j in range(batch)]\n",
    "        cur_w = lin_reg.W\n",
    "        grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        alpha = lrs_func(lr(lambda a: lin_reg.loss(lin_reg.W - a * grad_with_batch)), (i * batch) // len(lin_reg.X))\n",
    "        if is_nesterov:\n",
    "            cur_w -= alpha * beta_1 * V\n",
    "            grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        V = (beta_1 * V) + (1 - beta_1) * grad_with_batch\n",
    "        S = (beta_2 * S) + (1 - beta_2) * (grad_with_batch ** 2) if ~is_adagrad else (S + (grad_with_batch ** 2))\n",
    "        V_norm = V / (1 - (beta_1 ** (i + 1))) if is_corr_beta_1 else V\n",
    "        S_norm = S / (1 - (beta_2 ** (i + 1))) if is_corr_beta_2 else S\n",
    "\n",
    "        if without_squares:\n",
    "            lin_reg.W = lin_reg.W - alpha * V_norm\n",
    "        else:\n",
    "            lin_reg.W = lin_reg.W - alpha * (V_norm / (((S_norm) + eps_adam) ** 0.5))\n",
    "\n",
    "        if store_points:\n",
    "            lin_reg.W_points.append(np.copy(lin_reg.W))\n",
    "        lin_reg.loss_values.append(lin_reg.loss(lin_reg.W))\n",
    "        if abs(lin_reg.loss(lin_reg.W) - prev_W) < eps or i >= max_num_of_step:\n",
    "            break\n",
    "        prev_W = lin_reg.loss(lin_reg.W)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def sgd_handler(lin_reg, lr, method, lrs=LearningRateScheduling.Classic, batch=1, beta_1=0.9, beta_2=0.999,\n",
    "                eps_adam=10 ** -8,\n",
    "                eps=0.001, max_num_of_step=10000, store_points=False):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, without_squares=True, store_points=store_points)\n",
    "        case Methods.Momentum:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, without_squares=True, store_points=store_points)\n",
    "        case Methods.AdaGrad:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=0.5, eps_adam=eps_adam,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, is_adagrad=True, store_points=store_points)\n",
    "        case Methods.RMSprop:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=beta_2, eps_adam=eps_adam,\n",
    "                       is_corr_beta_1=False, store_points=store_points)\n",
    "        case Methods.Adam:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam,\n",
    "                       store_points=store_points)\n",
    "        case Methods.Nesterov:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, is_nesterov=True, without_squares=True,\n",
    "                       store_points=store_points)\n",
    "\n",
    "\n",
    "def lrs_exp(decay, epoch_update):\n",
    "    return lambda lr, t: lr / (decay ** (t // epoch_update))\n",
    "\n",
    "\n",
    "def lrs_step(decay, epoch_update):\n",
    "    return lambda lr, t: lr - decay * (t // epoch_update)\n",
    "\n",
    "\n",
    "def lrs_handler(lrs, epoch_update=10):\n",
    "    match lrs:\n",
    "        case LearningRateScheduling.Classic:\n",
    "            return lambda lr, t: lr\n",
    "        case LearningRateScheduling.Stepwise:\n",
    "            return lrs_step(0.001, epoch_update)\n",
    "        case LearningRateScheduling.Exponential:\n",
    "            return lrs_exp(2, epoch_update)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T02:18:10.098209Z",
     "end_time": "2023-04-16T02:18:10.280900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualise_points(lin_reg):\n",
    "    x = np.linspace(min(lin_reg.X), max(lin_reg.X), 1000)\n",
    "    y = sum(\n",
    "        [lin_reg.W[i] * (x ** i) for i in range(len(lin_reg.W))]\n",
    "    )\n",
    "    analys_w = lin_reg.analytical_solution()\n",
    "    analys_y = sum(\n",
    "        [analys_w[i] * (x ** i) for i in range(len(analys_w))]\n",
    "    )\n",
    "    plt.plot(x, y, '-r')\n",
    "    plt.plot(x, analys_y, '-b')\n",
    "    plt.plot(lin_reg.X, lin_reg.Y, 'og', linestyle='None')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualise_linear_sgd(lin_reg):\n",
    "    values = np.reshape(lin_reg.W_points, (2, len(lin_reg.W_points)))\n",
    "    X = np.linspace(min(values[0]) / 2, max(values[0]) * 2, 100)\n",
    "    Y = np.linspace(min(values[1]) / 2, max(values[1]) * 2, 100)\n",
    "    Z = [[lin_reg.loss(np.array((X[i], Y[j]))) for i in range(len(X))] for j in range(len(Y))]\n",
    "    plt.contour(X, Y, Z, 40)\n",
    "\n",
    "    plt.plot(values[0], values[1], marker='.')\n",
    "    plt.plot(values[0][0], values[1][0], 'og')\n",
    "    plt.plot(values[0][-1], values[1][-1], 'or')\n",
    "    plt.xlabel('w_1')\n",
    "    plt.ylabel('w_2')\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T02:18:10.320914Z",
     "end_time": "2023-04-16T02:18:10.348899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "\n",
    "def poly_array(coeffs):\n",
    "    return [lambda x, i=i: coeffs[i] * (x ** i) for i in range(len(coeffs))]\n",
    "\n",
    "\n",
    "def poly(poly_arr):\n",
    "    return lambda x: sum([poly_arr[i](x) for i in range(len(poly_arr))])\n",
    "\n",
    "\n",
    "def generate_data(num_of_points, dimension, coeffs_left, coeffs_right, x_left, x_right, deviation):\n",
    "    coeffs = np.array([float(random.randint(coeffs_left, coeffs_right)) for i in range(dimension + 1)])\n",
    "\n",
    "    X = [random.uniform(x_left, x_right) for _ in range(num_of_points)]\n",
    "    Y = [poly(poly_array(coeffs))(X[i]) + random.uniform(-deviation, +deviation) for i in range(num_of_points)]\n",
    "\n",
    "    return [np.array(X), np.array(Y), coeffs]\n",
    "\n",
    "\n",
    "def gen_linear_reg(dimension, num_of_points, coeffs_left, coeffs_right, x_left, x_right, deviation):\n",
    "    T = np.array(poly_array(np.ones(dimension + 1)))\n",
    "    X, Y, coeffs = generate_data(num_of_points, dimension, coeffs_left, coeffs_right, x_left, x_right, deviation)\n",
    "    W = np.ones(len(coeffs))\n",
    "\n",
    "    return LinearRegression(T, W, X, Y)\n",
    "\n",
    "\n",
    "def test_universal(lin_reg, lr, method, lrs, batch=1):\n",
    "    res_univ = {\n",
    "        'mem': 0,\n",
    "        'steps': 0,\n",
    "        'time': 0,\n",
    "        'sqrt': 0\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "    steps = sgd_handler(lin_reg, lr, method, lrs=lrs, batch=batch)\n",
    "    res_univ['mem'] = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    end = time.time()\n",
    "\n",
    "    res_univ['steps'] = (steps)\n",
    "    res_univ['time'] = end - start\n",
    "    res_univ['sqrt'] = lin_reg.loss(lin_reg.W, is_avarage=True)\n",
    "\n",
    "    return res_univ\n",
    "\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "\n",
    "def tests():\n",
    "    for dimension in range(1, 11):\n",
    "        for num_of_points in range(10, 100, 10):\n",
    "            for deviation in [0. + i for i in range(6)]:\n",
    "                lin_reg = gen_linear_reg(dimension=dimension, num_of_points=num_of_points, coeffs_left=-3,\n",
    "                                         coeffs_right=2, x_left=1., x_right=2., deviation=deviation)\n",
    "                lin_reg_info = 'Linear Regression Info: \\n Dimension: {} | Number of Points: {} | Deviation: {}' \\\n",
    "                    .format(dimension, num_of_points, deviation)\n",
    "                for method in Methods:\n",
    "                    for regularization in Regularization:\n",
    "                        for lr in LearningRate:\n",
    "                            for lrs in LearningRateScheduling:\n",
    "                                for batch in range(1, len(lin_reg.W) + 1):\n",
    "                                    for lr0 in [0.1, 0.01, 0.001]:\n",
    "                                        cur_lr = lambda x: lr0\n",
    "                                        sgd_info = 'SGD Info: \\n Batch: {} | Method: {} | Regularization: {} | Learning Rate(lr0={}): {} | Learning Rate Scheduling: {}'\\\n",
    "                                            .format(batch, method.name, regularization.name, lr0, lr.name, lrs.name)\n",
    "\n",
    "                                        lin_reg.W = np.ones(len(lin_reg.W))\n",
    "                                        lin_reg.W_points = [np.copy(lin_reg.W)]\n",
    "                                        lin_reg.loss_values = [lin_reg.loss(lin_reg.W)]\n",
    "                                        results = test_universal(lin_reg, cur_lr, method, lrs, batch)\n",
    "\n",
    "                                        print(lin_reg_info)\n",
    "                                        print(sgd_info)\n",
    "                                        print('Metrics: \\n ' + str(results))\n",
    "                                        visualise_points(lin_reg)\n",
    "                                    # visualise_linear_sgd(lin_reg) # only if dimension == 1\n",
    "                                    # test_results[info] = results\n",
    "\n",
    "\n",
    "tests()\n",
    "\n",
    "for key in test_results:\n",
    "    print(str(key) + \" -> \")\n",
    "    print('\\n'.join(str(val) for val in test_results[key]))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T00:04:06.838266Z",
     "end_time": "2023-04-16T00:04:38.846763Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
