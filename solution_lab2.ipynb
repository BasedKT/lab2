{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "Methods = Enum('Methods', ['Classic', 'Momentum', 'AdaGrad', 'RMSprop', 'Adam', 'Nesterov'])\n",
    "Regularization = Enum('Regularization', ['WithoutRegularization', 'L1', 'L2', 'Elastic'])\n",
    "LearningRate = Enum('LearningRate', ['Const'])\n",
    "LearningRateScheduling = Enum('LearningRateScheduling', ['Classic', 'Stepwise', 'Exponential'])\n",
    "\n",
    "\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, T, W, X, Y, regularization=Regularization.WithoutRegularization, l1=0.1, l2=0.1):\n",
    "        self.T = np.array([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))]).reshape(len(X), len(T))\n",
    "        self.W = W\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.regularization = regularization\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.W_points = [np.copy(self.W)]\n",
    "        self.loss_values = [self.loss(self.W)]\n",
    "\n",
    "    def loss(self, W_Arg):\n",
    "        val = sum([(np.dot(self.T[i], W_Arg) - self.Y[i]) ** 2 for i in range(len(self.X))])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                val += self.l1 * sum([abs(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                val += self.l2 * sum([w ** 2 for w in self.W]) / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                val += (self.l1 * sum([abs(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * sum([w ** 2 for w in self.W])) / len(self.W)\n",
    "\n",
    "        return val\n",
    "\n",
    "    def grad_by_components(self, index_components, W_Arg):\n",
    "        grad_with_batch = np.zeros(len(W_Arg))\n",
    "        for i in index_components:\n",
    "            grad_with_batch += (2 * (np.dot(self.T[i], W_Arg) - self.Y[i]) * self.T[i])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                grad_with_batch += self.l1 * np.array([sign(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                grad_with_batch += self.l2 * 2 * self.W / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                grad_with_batch += (self.l1 * np.array([sign(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * 2 * self.W) / len(self.W)\n",
    "\n",
    "        return grad_with_batch\n",
    "\n",
    "\n",
    "def sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam, is_corr_beta_1=True,\n",
    "        is_corr_beta_2=True, is_nesterov=False, decay=0.95, is_adagrad=False, without_squares=False,\n",
    "        store_points=False):\n",
    "    i = 0\n",
    "    prev_W = lin_reg.loss(lin_reg.W)\n",
    "    V = np.zeros(len(lin_reg.W))\n",
    "    S = np.zeros(len(lin_reg.W))\n",
    "    lrs_func = lrs_handler(lrs, decay)\n",
    "\n",
    "    while True:\n",
    "        i += 1\n",
    "\n",
    "        components = [(i * batch + j) % len(lin_reg.X) for j in range(batch)]\n",
    "        cur_w = lin_reg.W\n",
    "        grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        alpha = lrs_func(lr(lambda a: lin_reg.loss(lin_reg.W - a * grad_with_batch)), (i * batch) // len(lin_reg.X))\n",
    "        if is_nesterov:\n",
    "            cur_w -= alpha * beta_1 * V\n",
    "            grad_with_batch = lin_reg.grad_by_components(components, cur_w)\n",
    "\n",
    "        V = (beta_1 * V) + (1 - beta_1) * grad_with_batch\n",
    "        S = (beta_2 * S) + (1 - beta_2) * (grad_with_batch ** 2) if ~is_adagrad else (S + (grad_with_batch ** 2))\n",
    "        V_norm = V / (1 - (beta_1 ** (i + 1))) if is_corr_beta_1 else V\n",
    "        S_norm = S / (1 - (beta_2 ** (i + 1))) if is_corr_beta_2 else S\n",
    "\n",
    "        if without_squares:\n",
    "            lin_reg.W = lin_reg.W - alpha * V_norm\n",
    "        else:\n",
    "            lin_reg.W = lin_reg.W - alpha * (V_norm / (((S_norm) + eps_adam) ** 0.5))\n",
    "\n",
    "        if store_points:\n",
    "            lin_reg.W_points.append(np.copy(lin_reg.W))\n",
    "        lin_reg.loss_values.append(lin_reg.loss(lin_reg.W))\n",
    "        if abs(lin_reg.loss(lin_reg.W) - prev_W) < eps or i >= max_num_of_step:\n",
    "            break\n",
    "        prev_W = lin_reg.loss(lin_reg.W)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def sgd_handler(lin_reg, lr, method, lrs=LearningRateScheduling.Classic, batch=1, beta_1=0.9, beta_2=0.999,\n",
    "                eps_adam=10 ** -8,\n",
    "                eps=0.001, max_num_of_step=10000, store_points=False):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, without_squares=True, store_points=store_points)\n",
    "        case Methods.Momentum:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, without_squares=True, store_points=store_points)\n",
    "        case Methods.AdaGrad:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=0.5, eps_adam=eps_adam,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, is_adagrad=True, store_points=store_points)\n",
    "        case Methods.RMSprop:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1=0, beta_2=beta_2, eps_adam=eps_adam,\n",
    "                       is_corr_beta_1=False, store_points=store_points)\n",
    "        case Methods.Adam:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam,\n",
    "                       store_points=store_points)\n",
    "        case Methods.Nesterov:\n",
    "            return sgd(lin_reg, lr, lrs, eps, batch, max_num_of_step, beta_1, beta_2=1, eps_adam=1,\n",
    "                       is_corr_beta_1=False, is_corr_beta_2=False, is_nesterov=True, without_squares=True,\n",
    "                       store_points=store_points)\n",
    "\n",
    "\n",
    "def lrs_exp(decay):\n",
    "    return lambda lr, t: lr * (decay ** t)\n",
    "\n",
    "\n",
    "def lrs_step(decay):\n",
    "    return lambda lr, t: lr / (1 + decay * t)\n",
    "\n",
    "\n",
    "def lrs_handler(lrs, decay=0.99):\n",
    "    match lrs:\n",
    "        case LearningRateScheduling.Classic:\n",
    "            return lambda lr, t: lr\n",
    "        case LearningRateScheduling.Stepwise:\n",
    "            return lrs_step(decay)\n",
    "        case LearningRateScheduling.Exponential:\n",
    "            return lrs_exp(decay)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T21:40:33.408981Z",
     "end_time": "2023-04-15T21:40:33.480961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualise_points(linear_reg):\n",
    "    x = np.linspace(min(linear_reg.X), max(linear_reg.X), 1000)\n",
    "    y = sum(\n",
    "        [linear_reg.W[i] * (x ** i) for i in range(len(linear_reg.W))]\n",
    "    )\n",
    "    plt.plot(x, y, '-r')\n",
    "    plt.plot(linear_reg.X, linear_reg.Y, 'og', linestyle='None')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualise_linear_sgd(lin_reg):\n",
    "    values = np.reshape(lin_reg.W_points, (2, len(lin_reg.W_points)))\n",
    "    X = np.linspace(min(values[0]) / 2, max(values[0]) * 2, 100)\n",
    "    Y = np.linspace(min(values[1]) / 2, max(values[1]) * 2, 100)\n",
    "    Z = [[lin_reg.loss(np.array((X[i], Y[j]))) for i in range(len(X))] for j in range(len(Y))]\n",
    "    plt.contour(X, Y, Z, 40)\n",
    "\n",
    "    plt.plot(values[0], values[1], marker='.')\n",
    "    plt.plot(values[0][0], values[1][0], 'og')\n",
    "    plt.plot(values[0][-1], values[1][-1], 'or')\n",
    "    plt.xlabel('w_1')\n",
    "    plt.ylabel('w_2')\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T21:40:33.480961Z",
     "end_time": "2023-04-15T21:40:33.537008Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "\n",
    "def poly_array(coeffs):\n",
    "    return [lambda x, i=i: coeffs[i] * (x ** i) for i in range(len(coeffs))]\n",
    "\n",
    "\n",
    "def poly(functions):\n",
    "    return lambda x: sum([functions[i](x) for i in range(len(functions))])\n",
    "\n",
    "\n",
    "def generate_data(num_of_points, dimension, coeffs_left, coeffs_right, x_left, x_right, deviation):\n",
    "    coeffs = np.array([float(random.randint(coeffs_left, coeffs_right)) for i in range(dimension + 1)])\n",
    "\n",
    "    X = [random.uniform(x_left, x_right) for _ in range(num_of_points)]\n",
    "    Y = [poly(poly_array(coeffs))(X[i]) + random.uniform(-deviation, +deviation) for i in range(num_of_points)]\n",
    "\n",
    "    return [np.array(X), np.array(Y), coeffs]\n",
    "\n",
    "\n",
    "def gen_linear_reg(dimension, num_of_points, coeffs_left, coeffs_right, x_left, x_right, deviation):\n",
    "    T = np.array(poly_array(np.ones(dimension + 1)))\n",
    "    X, Y, coeffs = generate_data(num_of_points, dimension, coeffs_left, coeffs_right, x_left, x_right, deviation)\n",
    "    W = np.zeros(len(coeffs))\n",
    "\n",
    "    return LinearRegression(T, W, X, Y)\n",
    "\n",
    "\n",
    "def test_universal(lin_reg, lr, method, lrs):\n",
    "    # 1 - mem, 2 - steps, 3 - time, 4 - sqrs\n",
    "    res_univ = []\n",
    "\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "    steps = sgd_handler(lin_reg, lr, method, lrs=lrs, store_points=True)\n",
    "    res_univ.append(tracemalloc.get_traced_memory())\n",
    "    tracemalloc.stop()\n",
    "    end = time.time()\n",
    "\n",
    "    res_univ.append(steps)\n",
    "    res_univ.append(end - start)\n",
    "    res_univ.append(lin_reg.loss(lin_reg.W) / len(lin_reg.X))\n",
    "\n",
    "    return res_univ\n",
    "\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "\n",
    "def tests():\n",
    "    lin_reg = gen_linear_reg(dimension=1, num_of_points=60, coeffs_left=-4, coeffs_right=6, x_left=-2., x_right=1.,\n",
    "                             deviation=0.)\n",
    "    for method in Methods:\n",
    "        for regularization in Regularization:\n",
    "            for lr in LearningRate:\n",
    "                for lrs in LearningRateScheduling:\n",
    "                    cur_lr = lambda x: 0.01\n",
    "                    info = 'Method: {} | Regularization: {} | Learning Rate: {} | Learning Rate Scheduling: {}' \\\n",
    "                        .format(method.name, regularization.name, lr.name, lrs.name)\n",
    "\n",
    "                    lin_reg.W = np.ones(len(lin_reg.W))\n",
    "                    lin_reg.W_points = [np.copy(lin_reg.W)]\n",
    "                    lin_reg.loss_values = [lin_reg.loss(lin_reg.W)]\n",
    "                    results = test_universal(lin_reg, cur_lr, method, lrs)\n",
    "\n",
    "                    print(info)\n",
    "                    print(lin_reg.loss_values[-1])\n",
    "                    visualise_points(lin_reg)\n",
    "                    # visualise_linear_sgd(lin_reg) # only if dimension == 1\n",
    "                    test_results[info] = results\n",
    "\n",
    "\n",
    "tests()\n",
    "\n",
    "for key in test_results:\n",
    "    print(str(key) + \" -> \")\n",
    "    print('\\n'.join(str(val) for val in test_results[key]))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T21:39:27.074271Z",
     "end_time": "2023-04-15T21:39:45.922545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# n = 3\n",
    "#\n",
    "# current_t = np.array(np.array([power_lambda(n - 1 - i) for i in range(n)]))\n",
    "# current_w = np.zeros(n)\n",
    "# current_x = np.array([1., 2., 9., -2., -10.])\n",
    "# current_y = np.array([1., 2., 9., -2., 5])\n",
    "#\n",
    "# for method in Methods:\n",
    "#     for regularization in Regularization:\n",
    "#         for lr in LearningRate:\n",
    "#             linear_reg_const = LinearRegression(\n",
    "#                 current_t, current_w, current_x, current_y, regularization\n",
    "#             )\n",
    "#             print(str(method) + \" \" + str(regularization) + \" \" + str(lr))\n",
    "#             if lr == LearningRate.Const:\n",
    "#                 sgd_handler(linear_reg_const, lambda x: 0.01, method)\n",
    "#             elif lr == LearningRate.Dichotomy:\n",
    "#                 sgd_handler(linear_reg_const, lr_dichotomy(0.001, 0.0001), method)\n",
    "#             visualise_points(linear_reg_const)\n",
    "#             visualise_loss(np.array(linear_reg_const.loss_values))\n",
    "#             current_w = np.zeros(n)\n",
    "#             print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T21:39:45.930456Z",
     "end_time": "2023-04-15T21:39:45.969771Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
