{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "Methods = Enum('Methods', ['Classic', 'Momentum', 'AdaGrad', 'RMSprop', 'Adam', 'Nesterov'])\n",
    "Regularization = Enum('Regularization', ['WithoutRegularization', 'L1', 'L2', 'Elastic'])\n",
    "LearningRate = Enum('LearningRate', ['Const', 'Dichotomy'])\n",
    "\n",
    "\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, T, W, X, Y, regularization=Regularization.WithoutRegularization, l1=0.1, l2=0.1):\n",
    "        self.T = np.array([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))]).reshape(len(X), len(T))\n",
    "        self.W = W\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.regularization = regularization\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def loss_function_value(self, W_Arg):\n",
    "        val = sum([(np.dot(self.T[i], W_Arg) - self.Y[i]) ** 2 for i in range(len(self.X))])\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                val += self.l1 * sum([abs(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                val += self.l2 * sum([w ** 2 for w in self.W]) / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                val += (self.l1 * sum([abs(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * sum([w ** 2 for w in self.W])) / len(self.W)\n",
    "        return val\n",
    "\n",
    "    def grad_by_components(self, index_components, W_Arg):\n",
    "        grad_with_batch = np.zeros(len(W_Arg))\n",
    "        for i in index_components:\n",
    "            grad_with_batch += 2 * (np.dot(self.T[i], W_Arg) - self.Y[i]) * self.T[i]\n",
    "        match self.regularization:\n",
    "            case Regularization.L1:\n",
    "                grad_with_batch += self.l1 * np.array([sign(w) for w in self.W]) / len(self.W)\n",
    "            case Regularization.L2:\n",
    "                grad_with_batch += self.l2 * 2 * self.W / len(self.W)\n",
    "            case Regularization.Elastic:\n",
    "                grad_with_batch += (self.l1 * np.array([sign(w) for w in self.W])) / len(self.W) + (\n",
    "                        self.l2 * 2 * self.W) / len(self.W)\n",
    "\n",
    "        return grad_with_batch\n",
    "\n",
    "\n",
    "def sgd(lin_reg, lr, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam, is_corr_beta_1=True, is_corr_beta_2=True,\n",
    "        is_nesterov=False):\n",
    "    i = 0\n",
    "    prev_W = lin_reg.loss_function_value(lin_reg.W)\n",
    "    V = np.zeros(len(lin_reg.W))\n",
    "    S = np.zeros(len(lin_reg.W))\n",
    "    while True:\n",
    "        components = [(i * batch + j) % len(lin_reg.X) for j in range(batch)]\n",
    "        grad_with_batch = lin_reg.grad_by_components(components, lin_reg.W)\n",
    "        alpha = lr(lambda a: lin_reg.loss_function_value(lin_reg.W - a * grad_with_batch))\n",
    "        V = (beta_1 * V) + (1 - beta_1) * grad_with_batch if ~is_nesterov else (beta_1 * V) + (\n",
    "                1 - beta_1) * lin_reg.grad_by_components(lin_reg.W - alpha * beta_1 * V)\n",
    "        S = (beta_2 * S) + (1 - beta_2) * (grad_with_batch ** 2)\n",
    "        V_norm = V / (1 - (beta_1 ** (i + 1))) if is_corr_beta_1 else V\n",
    "        S_norm = S / (1 - (beta_2 ** (i + 1))) if is_corr_beta_2 else S\n",
    "        lin_reg.W -= alpha * (V_norm / (((S_norm) + eps_adam) ** 0.5))\n",
    "        if abs(lin_reg.loss_function_value(lin_reg.W) - prev_W) < eps or i >= max_num_of_step:\n",
    "            break\n",
    "        prev_W = lin_reg.loss_function_value(lin_reg.W)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def sgd_handler(lin_reg, lr, method, batch=1, beta_1=0.9, beta_2=0.999, eps_adam=10 ** -8, eps=0.001,\n",
    "                max_num_of_step=10000):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            sgd(lin_reg, lr, eps, batch, max_num_of_step, 0, 1, 1, False, False)\n",
    "        case Methods.Momentum:\n",
    "            sgd(lin_reg, lr, eps, batch, max_num_of_step, beta_1, 1, 1, False, False)\n",
    "        case Methods.AdaGrad:\n",
    "            sgd(lin_reg, lr, eps, batch, max_num_of_step, 0, 0, eps_adam, False, False)\n",
    "        case Methods.RMSprop:\n",
    "            sgd(lin_reg, lr, eps, batch, max_num_of_step, 0, beta_2, eps_adam, False)\n",
    "        case Methods.Adam:\n",
    "            sgd(lin_reg, lr, eps, batch, max_num_of_step, beta_1, beta_2, eps_adam)\n",
    "        case Methods.Nesterov:\n",
    "            sgd(lin_reg, lr, eps, batch, max_num_of_step, beta_1, 1, 1, False, False, True)\n",
    "\n",
    "\n",
    "def lr_dichotomy(eps, delt):\n",
    "    return lambda lin_reg: dichotomy(lin_reg, 0, right_border_calc(lin_reg), eps, delt)\n",
    "\n",
    "\n",
    "def right_border_calc(func):\n",
    "    right_start = 0.0000001\n",
    "    zero = func(0.)\n",
    "    while zero >= func(right_start):\n",
    "        right_start *= 1.3\n",
    "\n",
    "    return right_start\n",
    "\n",
    "\n",
    "def dichotomy(func, a_1, a_2, eps, delt):\n",
    "    while abs(a_1 - a_2) >= eps:\n",
    "        new_a_1 = (a_1 + a_2) / 2 - delt\n",
    "        new_a_2 = (a_1 + a_2) / 2 + delt\n",
    "        fv1 = func(new_a_1)\n",
    "        fv2 = func(new_a_2)\n",
    "        if fv2 > fv1:\n",
    "            a_2 = new_a_2\n",
    "        elif fv2 < fv1:\n",
    "            a_1 = new_a_1\n",
    "        else:\n",
    "            a_1 = new_a_1\n",
    "            a_2 = new_a_2\n",
    "    return (a_1 + a_2) / 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T17:59:38.329646Z",
     "end_time": "2023-04-13T17:59:38.371803Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualise_points(linear_reg):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = linear_reg.W[0] * x + linear_reg.W[1]\n",
    "    plt.plot(x, y, '-r')\n",
    "    plt.plot(linear_reg.X, linear_reg.Y, 'og', linestyle='None')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.show()\n",
    "    print(linear_reg.W)\n",
    "    print(linear_reg.loss_function_value(linear_reg.W))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T17:59:38.369811Z",
     "end_time": "2023-04-13T17:59:39.888893Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_t = np.array([lambda x: x, lambda x: 1.])\n",
    "current_w = np.array([0., 0.])\n",
    "current_x = np.array([1., 2., 9., -2., -10.])\n",
    "current_y = np.array([1., 2., 9., -2., 5])\n",
    "\n",
    "for method in Methods:\n",
    "    for regularization in Regularization:\n",
    "        for lr in LearningRate:\n",
    "            linear_reg_const = LinearRegression(\n",
    "                current_t, current_w, current_x, current_y, regularization\n",
    "            )\n",
    "            print(str(method) + \" \" + str(regularization) + \" \" + str(lr))\n",
    "            if lr == LearningRate.Const:\n",
    "                sgd_handler(linear_reg_const, lambda x: 0.01, method)\n",
    "            elif lr == LearningRate.Dichotomy:\n",
    "                sgd_handler(linear_reg_const, lr_dichotomy(0.001, 0.0001), method)\n",
    "            visualise_points(linear_reg_const)\n",
    "            current_w = np.array([0., 0.])\n",
    "            print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T17:59:39.890154Z",
     "end_time": "2023-04-13T18:00:02.302143Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_y(x, coeffs):\n",
    "    return sum([coeffs[i] * (x ** i) for i in range(len(coeffs))])\n",
    "\n",
    "\n",
    "def generate_data(coeffs, num_of_points, x_range_left, x_range_right, deviation):\n",
    "    x_es = []\n",
    "    y_es = []\n",
    "    for i in range(num_of_points):\n",
    "        x_es.append(random.uniform(x_range_left, x_range_right))\n",
    "        y_es.append(calc_y(x_es[i], coeffs) + random.uniform(-deviation, +deviation))\n",
    "    return [np.array(x_es), np.array(y_es)]\n",
    "\n",
    "\n",
    "def gen_linear_reg(coeffs, num_of_points, x_range_left, x_range_right, deviation, calculated_lambdas):\n",
    "    t = np.array(calculated_lambdas)\n",
    "    points = generate_data(coeffs, num_of_points, x_range_left, x_range_right, deviation)\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    w = np.zeros(len(coeffs))\n",
    "    return LinearRegression(t, w, x, y)\n",
    "\n",
    "\n",
    "def test_universal(current_method, current_lr, linear_regression, start_coeffs):\n",
    "    # 1 - mem, 2 - steps, 3 - time, 4 - sqrs\n",
    "    res_univ = []\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "    steps = sgd_handler(linear_regression, current_lr, current_method)\n",
    "    res_univ.append(tracemalloc.get_traced_memory())\n",
    "    tracemalloc.stop()\n",
    "    end = time.time()\n",
    "    res_univ.append(steps)\n",
    "    res_univ.append(end - start)\n",
    "    calculated_coeffs = linear_regression.W\n",
    "    xes = linear_regression.X\n",
    "    yes = linear_regression.Y\n",
    "    calculated_vals = []\n",
    "    for i in range(len(xes)):\n",
    "        calculated_vals.append(calc_y(xes[i], start_coeffs))\n",
    "    sums = 0.\n",
    "    for i in range(len(xes)):\n",
    "        sums += (calculated_vals[i] - yes[i]) ** 2 / len(calculated_coeffs)\n",
    "    res_univ.append(sums)\n",
    "    return res_univ\n",
    "\n",
    "\n",
    "power_lambda = lambda power: lambda x: x ** power\n",
    "\n",
    "first_tests_val = gen_linear_reg(\n",
    "    coeffs=[24., -26., -15., 25., -9., 1.],\n",
    "    num_of_points=15,\n",
    "    x_range_left=1.,\n",
    "    x_range_right=4.,\n",
    "    deviation=0.0,\n",
    "    calculated_lambdas=[power_lambda(5 - i) for i in range(6)]\n",
    ")\n",
    "\n",
    "start_w_for_first = np.array([0., 0., 0., 0., 0., 0.])\n",
    "start_coeffs_for_first = [1., -9., 25., -15., -26., 24.]\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "\n",
    "def copy(lin_reg):\n",
    "    lin_reg.W = np.zeros(len(lin_reg.W))\n",
    "\n",
    "\n",
    "for method in Methods:\n",
    "    for regularization in Regularization:\n",
    "        for lr in LearningRate:\n",
    "            print(step)\n",
    "            step += 1\n",
    "            current_lr = None\n",
    "            lr_name = None\n",
    "            if lr == LearningRate.Const:\n",
    "                current_lr = lambda x: 0.01\n",
    "                lr_name = \"const\"\n",
    "            elif lr == LearningRate.Dichotomy:\n",
    "                current_lr = lr_dichotomy(0.001, 0.0001)\n",
    "                lr_name = \"dichotomy\"\n",
    "            results = test_universal(method, current_lr, first_tests_val, start_coeffs_for_first)\n",
    "            copy(lin_reg=first_tests_val)\n",
    "            test_results[\n",
    "                \"Method: \" + method.name + \" Regularization: \" + regularization.name + \" LR: \" + lr_name] = results\n",
    "\n",
    "for key in test_results:\n",
    "    print(str(key) + \" -> \")\n",
    "    for val in test_results[key]:\n",
    "        print(val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T18:00:02.265369Z",
     "end_time": "2023-04-13T18:00:02.362583Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
